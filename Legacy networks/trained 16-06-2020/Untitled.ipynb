{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# ------------ Parameters-------------\n",
    "def GetParams():\n",
    "    \n",
    "    opt = argparse.Namespace()\n",
    "    opt.model='rcan'\n",
    "    opt.lr=1e-4\n",
    "    opt.norm='hist'\n",
    "    opt.nepoch=30\n",
    "    opt.saveinterval=5\n",
    "    opt.modifyPretrainedModel=''\n",
    "    opt.multigpu=''\n",
    "    opt.undomulti=''\n",
    "    opt.ntrain=2400\n",
    "    opt.scheduler=''\n",
    "    opt.log='store_true'\n",
    "    opt.noise=''\n",
    "\n",
    "# data\n",
    "    opt.dataset='fouriersim'\n",
    "    opt.imageSize=512,\n",
    "    opt.weights='C:/Users/edmax/Edward W/ML python/ML-SIM/Training code/results'\n",
    "    opt.basedir=''\n",
    "    opt.root='E:/Users/ew535/Training datasets/Trained sets/11-06-2020/Training'\n",
    "    opt.server='E:/Users/ew535/Training datasets/Trained sets/11-06-2020/Training'\n",
    "    opt.local='E:/Users/ew535/Training datasets/Trained sets/11-06-2020/Training'\n",
    "    opt.out='results'\n",
    "\n",
    "    # computation \n",
    "    opt.workers=6\n",
    "    opt.batchSize=1\n",
    "    \n",
    "    # restoration options\n",
    "    opt.task='simin_gtout'\n",
    "    opt.scale=1\n",
    "    opt.nch_in=9\n",
    "    opt.nch_out=1\n",
    "    \n",
    "    # architecture options \n",
    "    opt.narch=0 \n",
    "    opt.n_resblocks=10\n",
    "    opt.n_resgroups=3\n",
    "    opt.reduction=4\n",
    "    opt.n_feats=48\n",
    "\n",
    "    # test options\n",
    "    opt.ntest=10\n",
    "    opt.testinterval=1\n",
    "    opt.test=''\n",
    "    opt.cpu=''\n",
    "    opt.batchSize_test=1\n",
    "    opt.plotinterval=5\n",
    "    \n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(basedir='', batchSize=1, batchSize_test=1, cpu='', dataset='fouriersim', fid=<_io.TextIOWrapper name='results/log.txt' mode='w' encoding='cp1252'>, imageSize=(512,), local='E:/Users/ew535/Training datasets/Trained sets/11-06-2020/Training', log='store_true', lr=0.0001, model='rcan', modifyPretrainedModel='', multigpu='', n_feats=48, n_resblocks=10, n_resgroups=3, narch=0, nch_in=9, nch_out=1, nepoch=30, noise='', norm='hist', ntest=10, ntrain=2400, out='results', plotinterval=5, reduction=4, root='E:/Users/ew535/Training datasets/Trained sets/11-06-2020/Training', saveinterval=5, scale=1, scheduler='', server='E:/Users/ew535/Training datasets/Trained sets/11-06-2020/Training', task='simin_gtout', test='', testinterval=1, undomulti='', weights='C:/Users/edmax/Edward W/ML python/ML-SIM/Training code/results/prelim.pth', workers=6)\n",
      "getting dataloader E:/Users/ew535/Training datasets/Trained sets/11-06-2020/Training\n",
      "not using normalization\n",
      "training\n",
      "Training\n",
      "loading checkpoint C:/Users/edmax/Edward W/ML python/ML-SIM/Training code/results/prelim.pth\n",
      "\n",
      "Learning rate 0.0001\n",
      "[11/30][2400/2400] Loss: 0.002067\n",
      "Epoch 10 done, 0.004942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmax\\Edward W\\ML python\\ML-SIM\\Training code\\plotting.py:26: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  PSNR = 20*np.log10(1/np.sqrt(MSE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 22.64 dB / 0.8665\n",
      "\n",
      "Learning rate 0.0001\n",
      "[12/30][2400/2400] Loss: 0.003128\n",
      "Epoch 11 done, 0.004757\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 25.97 dB / 0.8901\n",
      "\n",
      "Learning rate 0.0001\n",
      "[13/30][2400/2400] Loss: 0.011172\n",
      "Epoch 12 done, 0.004684\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 24.57 dB / 0.8886\n",
      "\n",
      "Learning rate 0.0001\n",
      "[14/30][2400/2400] Loss: 0.002038\n",
      "Epoch 13 done, 0.004406\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 25.53 dB / 0.8949\n",
      "\n",
      "Learning rate 0.0001\n",
      "[15/30][2400/2400] Loss: 0.001963\n",
      "Epoch 14 done, 0.004737\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 25.88 dB / 0.8773\n",
      "\n",
      "Learning rate 0.0001\n",
      "[16/30][2400/2400] Loss: 0.001766\n",
      "Epoch 15 done, 0.004523\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 25.29 dB / 0.9004\n",
      "\n",
      "Learning rate 0.0001\n",
      "[17/30][2400/2400] Loss: 0.009582\n",
      "Epoch 16 done, 0.004359\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 23.72 dB / 0.8258\n",
      "\n",
      "Learning rate 0.0001\n",
      "[18/30][2400/2400] Loss: 0.006871\n",
      "Epoch 17 done, 0.004041\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 24.62 dB / 0.9104\n",
      "\n",
      "Learning rate 0.0001\n",
      "[19/30][2400/2400] Loss: 0.004596\n",
      "Epoch 18 done, 0.004104\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 23.97 dB / 0.8953\n",
      "\n",
      "Learning rate 0.0001\n",
      "[20/30][2400/2400] Loss: 0.001208\n",
      "Epoch 19 done, 0.003879\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 27.85 dB / 0.9118\n",
      "\n",
      "Learning rate 0.0001\n",
      "[21/30][2400/2400] Loss: 0.005528\n",
      "Epoch 20 done, 0.003910\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 26.56 dB / 0.9090\n",
      "\n",
      "Learning rate 0.0001\n",
      "[22/30][2400/2400] Loss: 0.001561\n",
      "Epoch 21 done, 0.003926\n",
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 26.17 dB / 0.8976\n",
      "\n",
      "Learning rate 0.0001\n",
      "[23/30][289/2400] Loss: 0.024685"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import GetModel\n",
    "from datahandler import GetDataloaders\n",
    "\n",
    "from plotting import testAndMakeCombinedPlots\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "opt = GetParams()\n",
    "\n",
    "if opt.norm == '':\n",
    "    opt.norm = opt.dataset\n",
    "elif opt.norm.lower() == 'none':\n",
    "    opt.norm = None\n",
    "\n",
    "if len(opt.basedir) > 0:\n",
    "    opt.root = opt.root.replace('basedir', opt.basedir)\n",
    "    opt.weights = opt.weights.replace('basedir', opt.basedir)\n",
    "    opt.out = opt.out.replace('basedir', opt.basedir)\n",
    "\n",
    "if opt.out[:4] == 'root':\n",
    "    opt.out = opt.out.replace('root', opt.root)\n",
    "\n",
    "\n",
    "# convenience function\n",
    "if len(opt.weights) > 0 and not os.path.isfile(opt.weights):\n",
    "    # folder provided, trying to infer model options\n",
    "\n",
    "    logfile = opt.weights + '/log.txt'\n",
    "    opt.weights += '/final.pth'\n",
    "    if not os.path.isfile(opt.weights):\n",
    "        opt.weights = opt.weights.replace('final.pth', 'prelim.pth')\n",
    "\n",
    "    if os.path.isfile(logfile):\n",
    "        fid = open(logfile, 'r')\n",
    "        optstr = fid.read()\n",
    "        optlist = optstr.split(', ')\n",
    "\n",
    "        def getopt(optname, typestr):\n",
    "            opt_e = [e.split('=')[-1].strip(\"\\'\")\n",
    "                     for e in optlist if (optname.split('.')[-1] + '=') in e]\n",
    "            return eval(optname) if len(opt_e) == 0 else typestr(opt_e[0])\n",
    "\n",
    "        opt.model = getopt('opt.model', str)\n",
    "        opt.task = getopt('opt.task', str)\n",
    "        opt.nch_in = getopt('opt.nch_in', int)\n",
    "        opt.nch_out = getopt('opt.nch_out', int)\n",
    "        opt.n_resgroups = getopt('opt.n_resgroups', int)\n",
    "        opt.n_resblocks = getopt('opt.n_resblocks', int)\n",
    "        opt.n_feats = getopt('opt.n_feats', int)\n",
    "\n",
    "\n",
    "def remove_dataparallel_wrapper(state_dict):\n",
    "    r\"\"\"Converts a DataParallel model to a normal one by removing the \"module.\"\n",
    "    wrapper in the module dictionary\n",
    "\n",
    "    Args:\n",
    "            state_dict: a torch.nn.DataParallel state dictionary\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, vl in state_dict.items():\n",
    "        name = k[7:]  # remove 'module.' of DataParallel\n",
    "        new_state_dict[name] = vl\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def train(dataloader, validloader, net, nepoch=10):\n",
    "    print('Training')\n",
    "    start_epoch = 0\n",
    "    if opt.task == 'segment':\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
    "    loss_function.cuda()\n",
    "    if len(opt.weights) > 0:  # load previous weights?\n",
    "        checkpoint = torch.load(opt.weights)\n",
    "        print('loading checkpoint', opt.weights)\n",
    "        if opt.undomulti:\n",
    "            checkpoint['state_dict'] = remove_dataparallel_wrapper(\n",
    "                checkpoint['state_dict'])\n",
    "        if opt.modifyPretrainedModel:\n",
    "            pretrained_dict = checkpoint['state_dict']\n",
    "            model_dict = net.state_dict()\n",
    "            # 1. filter out unnecessary keys\n",
    "            for k, v in list(pretrained_dict.items()):\n",
    "                print(k)\n",
    "            pretrained_dict = {k: v for k, v in list(\n",
    "                pretrained_dict.items())[:-2]}\n",
    "            # 2. overwrite entries in the existing state dict\n",
    "            model_dict.update(pretrained_dict)\n",
    "            # 3. load the new state dict\n",
    "            net.load_state_dict(model_dict)\n",
    "\n",
    "            # optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "        else:\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            if opt.lr == 1:  # continue as it was\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "\n",
    "\n",
    "        # if opt.modifyPretrainedModel:\n",
    "        #     mod = list(net.children())\n",
    "        #     mod.pop()\n",
    "        #     mod.append(nn.Conv2d(64, 2, 1))\n",
    "        #     net = torch.nn.Sequential(*mod)\n",
    "        #     net.cuda()\n",
    "        #     opt.task = 'segment'\n",
    "\n",
    "    if len(opt.scheduler) > 0:\n",
    "        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=5, min_lr=0, eps=1e-08)\n",
    "        stepsize, gamma = int(opt.scheduler.split(\n",
    "            ',')[0]), float(opt.scheduler.split(',')[1])\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, stepsize, gamma=gamma)\n",
    "        if 'scheduler' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    count = 0\n",
    "    opt.t0 = time.perf_counter()\n",
    "\n",
    "    for epoch in range(start_epoch, nepoch):\n",
    "        mean_loss = 0\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('\\nLearning rate', param_group['lr'])\n",
    "\n",
    "        # if len(opt.lrseq) > 0:\n",
    "        #     t = '[5,1e-4,10,1e-5]'\n",
    "        #     t = np.array(t)\n",
    "        #     epochvec = t[::2].astype('int')\n",
    "        #     lrvec = t[1::2].astype('float')\n",
    "\n",
    "        #     idx = epochvec.indexOf(epoch)\n",
    "        #     opt.lr = lrvec[idx]\n",
    "        #     optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
    "\n",
    "        for i, bat in enumerate(dataloader):\n",
    "            lr, hr = bat[0], bat[1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if opt.model == 'ffdnet':\n",
    "                stdvec = torch.zeros(lr.shape[0])\n",
    "                for j in range(lr.shape[0]):\n",
    "                    noise = lr[j] - hr[j]\n",
    "                    stdvec[j] = torch.std(noise)\n",
    "                noise = net(lr.cuda(), stdvec.cuda())\n",
    "                sr = torch.clamp(lr.cuda() - noise, 0, 1)\n",
    "                gt_noise = lr.cuda() - hr.cuda()\n",
    "                loss = loss_function(noise, gt_noise)\n",
    "            elif opt.task == 'residualdenoising':\n",
    "                noise = net(lr.cuda())\n",
    "                gt_noise = lr.cuda() - hr.cuda()\n",
    "                loss = loss_function(noise, gt_noise)\n",
    "            else:\n",
    "                sr = net(lr.cuda())\n",
    "                if opt.task == 'segment':\n",
    "                    if opt.nch_out > 2:\n",
    "                        hr_classes = torch.round((opt.nch_out+1)*hr).long()\n",
    "                        loss = loss_function(\n",
    "                            sr.squeeze(), hr_classes.squeeze().cuda())\n",
    "                    else:\n",
    "                        loss = loss_function(\n",
    "                            sr.squeeze(), hr.long().squeeze().cuda())\n",
    "                else:\n",
    "                    loss = loss_function(sr, hr.cuda())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ######### Status and display #########\n",
    "            mean_loss += loss.data.item()\n",
    "            print('\\r[%d/%d][%d/%d] Loss: %0.6f' % (epoch+1, nepoch,\n",
    "                                                    i+1, len(dataloader), loss.data.item()), end='')\n",
    "\n",
    "            count += 1\n",
    "            if opt.log and count*opt.batchSize // 1000 > 0:\n",
    "                t1 = time.perf_counter() - opt.t0\n",
    "                mem = torch.cuda.memory_allocated()\n",
    "                print(epoch, count*opt.batchSize, t1, mem,\n",
    "                      mean_loss / count, file=opt.train_stats)\n",
    "                opt.train_stats.flush()\n",
    "                count = 0\n",
    "\n",
    "\n",
    "        # ---------------- Scheduler -----------------\n",
    "        if len(opt.scheduler) > 0:\n",
    "            scheduler.step()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                print('\\nLearning rate', param_group['lr'])\n",
    "                break\n",
    "\n",
    "        # ---------------- Printing -----------------\n",
    "        print('\\nEpoch %d done, %0.6f' %\n",
    "              (epoch, (mean_loss / len(dataloader))))\n",
    "        print('\\nEpoch %d done, %0.6f' %\n",
    "              (epoch, (mean_loss / len(dataloader))), file=opt.fid)\n",
    "        opt.fid.flush()\n",
    "\n",
    "        # ---------------- TEST -----------------\n",
    "        if (epoch + 1) % opt.testinterval == 0:\n",
    "            testAndMakeCombinedPlots(net, validloader, opt, epoch)\n",
    "            # if opt.scheduler:\n",
    "            # scheduler.step(mean_loss / len(dataloader))\n",
    "\n",
    "        if (epoch + 1) % opt.saveinterval == 0:\n",
    "            # torch.save(net.state_dict(), opt.out + '/prelim.pth')\n",
    "            checkpoint = {'epoch': epoch + 1,\n",
    "                          'state_dict': net.state_dict(),\n",
    "                          'optimizer': optimizer.state_dict()}\n",
    "            if len(opt.scheduler) > 0:\n",
    "                checkpoint['scheduler'] = scheduler.state_dict()\n",
    "            torch.save(checkpoint, '%s/prelim%d.pth' % (opt.out, epoch+1))\n",
    "\n",
    "    checkpoint = {'epoch': nepoch,\n",
    "                  'state_dict': net.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict()}\n",
    "    if len(opt.scheduler) > 0:\n",
    "        checkpoint['scheduler'] = scheduler.state_dict()\n",
    "    torch.save(checkpoint, opt.out + '/final.pth')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    try:\n",
    "        os.makedirs(opt.out)\n",
    "    except IOError:\n",
    "        pass\n",
    "\n",
    "    opt.fid = open(opt.out + '/log.txt', 'w')\n",
    "    print(opt)\n",
    "    print(opt, '\\n', file=opt.fid)\n",
    "    print('getting dataloader', opt.root)\n",
    "    dataloader, validloader = GetDataloaders(opt)\n",
    "    net = GetModel(opt)\n",
    "\n",
    "    if opt.log:\n",
    "        opt.train_stats = open(opt.out.replace(\n",
    "            '\\\\', '/') + '/train_stats.csv', 'w')\n",
    "        opt.test_stats = open(opt.out.replace(\n",
    "            '\\\\', '/') + '/test_stats.csv', 'w')\n",
    "        print('iter,nsample,time,memory,meanloss', file=opt.train_stats)\n",
    "        print('iter,time,memory,psnr,ssim', file=opt.test_stats)\n",
    "\n",
    "    import time\n",
    "    t0 = time.perf_counter()\n",
    "    if not opt.test:\n",
    "        print('training')\n",
    "        if opt.model.lower() == 'srgan':\n",
    "            GANtrain(dataloader, validloader, net, nepoch=opt.nepoch)\n",
    "        elif opt.model.lower() == 'esrgan':\n",
    "            ESRGANtrain(dataloader, validloader, net, nepoch=opt.nepoch)\n",
    "        else:\n",
    "            train(dataloader, validloader, net, nepoch=opt.nepoch)\n",
    "        torch.save(net.state_dict(), opt.out + '/final.pth')\n",
    "    else:\n",
    "        if len(opt.weights) > 0:  # load previous weights?\n",
    "            checkpoint = torch.load(opt.weights)\n",
    "            print('loading checkpoint', opt.weights)\n",
    "            if opt.undomulti:\n",
    "                checkpoint['state_dict'] = remove_dataparallel_wrapper(\n",
    "                    checkpoint['state_dict'])\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            print('time: ', time.perf_counter()-t0)\n",
    "        #testAndMakeCombinedPlots(net, validloader, opt)\n",
    "    print('time: ', time.perf_counter()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
