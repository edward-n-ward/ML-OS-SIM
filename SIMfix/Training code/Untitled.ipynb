{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# ------------ Parameters-------------\n",
    "def GetParams():\n",
    "    \n",
    "    opt = argparse.Namespace()\n",
    "    opt.model='rcan'\n",
    "    opt.lr=1e-4\n",
    "    opt.norm='hist'\n",
    "    opt.nepoch=30\n",
    "    opt.saveinterval=1\n",
    "    opt.modifyPretrainedModel=''\n",
    "    opt.multigpu=''\n",
    "    opt.undomulti=''\n",
    "    opt.ntrain=4000\n",
    "    opt.scheduler=''\n",
    "    opt.log='store_true'\n",
    "    opt.noise=''\n",
    "\n",
    "# data\n",
    "    opt.dataset='fouriersim'\n",
    "    opt.imageSize=512,\n",
    "    opt.weights=''\n",
    "    opt.basedir=''\n",
    "    opt.root='E:/Users/ew535/Training datasets/Trained sets/16-06-2020'\n",
    "    opt.server='E:/Users/ew535/Training datasets/Trained sets/16-06-2020'\n",
    "    opt.local='E:/Users/ew535/Training datasets/Trained sets/16-06-2020'\n",
    "    opt.out='results'\n",
    "\n",
    "    # computation \n",
    "    opt.workers=6\n",
    "    opt.batchSize=1\n",
    "    \n",
    "    # restoration options\n",
    "    opt.task='simin_gtout'\n",
    "    opt.scale=1\n",
    "    opt.nch_in=9\n",
    "    opt.nch_out=1\n",
    "    \n",
    "    # architecture options \n",
    "    opt.narch=0 \n",
    "    opt.n_resblocks=10\n",
    "    opt.n_resgroups=3\n",
    "    opt.reduction=4\n",
    "    opt.n_feats=48\n",
    "\n",
    "    # test options\n",
    "    opt.ntest=10\n",
    "    opt.testinterval=1\n",
    "    opt.test=''\n",
    "    opt.cpu=''\n",
    "    opt.batchSize_test=6\n",
    "    opt.plotinterval=5\n",
    "    \n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(basedir='', batchSize=1, batchSize_test=6, cpu='', dataset='fouriersim', fid=<_io.TextIOWrapper name='results/log.txt' mode='w' encoding='cp1252'>, imageSize=(512,), local='E:/Users/ew535/Training datasets/Trained sets/16-06-2020', log='store_true', lr=0.0001, model='rcan', modifyPretrainedModel='', multigpu='', n_feats=48, n_resblocks=10, n_resgroups=3, narch=0, nch_in=9, nch_out=1, nepoch=30, noise='', norm='hist', ntest=10, ntrain=4000, out='results', plotinterval=5, reduction=4, root='E:/Users/ew535/Training datasets/Trained sets/16-06-2020', saveinterval=1, scale=1, scheduler='', server='E:/Users/ew535/Training datasets/Trained sets/16-06-2020', task='simin_gtout', test='', testinterval=1, undomulti='', weights='', workers=6)\n",
      "getting dataloader E:/Users/ew535/Training datasets/Trained sets/16-06-2020\n",
      "not using normalization\n",
      "training\n",
      "Training\n",
      "\n",
      "Learning rate 0.0001\n",
      "[1/30][4000/4000] Loss: 0.003499\n",
      "Epoch 0 done, 0.013409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmax\\Edward W\\ML python\\ML-SIM\\Training code\\plotting.py:26: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  PSNR = 20*np.log10(1/np.sqrt(MSE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing of 10 samples complete. bc: inf dB / 1.0000, sr: 20.41 dB / 0.8228\n",
      "\n",
      "Learning rate 0.0001\n",
      "[2/30][354/4000] Loss: 0.003762"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import GetModel\n",
    "from datahandler import GetDataloaders\n",
    "\n",
    "from plotting import testAndMakeCombinedPlots\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "opt = GetParams()\n",
    "\n",
    "if opt.norm == '':\n",
    "    opt.norm = opt.dataset\n",
    "elif opt.norm.lower() == 'none':\n",
    "    opt.norm = None\n",
    "\n",
    "if len(opt.basedir) > 0:\n",
    "    opt.root = opt.root.replace('basedir', opt.basedir)\n",
    "    opt.weights = opt.weights.replace('basedir', opt.basedir)\n",
    "    opt.out = opt.out.replace('basedir', opt.basedir)\n",
    "\n",
    "if opt.out[:4] == 'root':\n",
    "    opt.out = opt.out.replace('root', opt.root)\n",
    "\n",
    "\n",
    "# convenience function\n",
    "if len(opt.weights) > 0 and not os.path.isfile(opt.weights):\n",
    "    # folder provided, trying to infer model options\n",
    "\n",
    "    logfile = opt.weights + '/log.txt'\n",
    "    opt.weights += '/final.pth'\n",
    "    if not os.path.isfile(opt.weights):\n",
    "        opt.weights = opt.weights.replace('final.pth', 'prelim.pth')\n",
    "\n",
    "    if os.path.isfile(logfile):\n",
    "        fid = open(logfile, 'r')\n",
    "        optstr = fid.read()\n",
    "        optlist = optstr.split(', ')\n",
    "\n",
    "        def getopt(optname, typestr):\n",
    "            opt_e = [e.split('=')[-1].strip(\"\\'\")\n",
    "                     for e in optlist if (optname.split('.')[-1] + '=') in e]\n",
    "            return eval(optname) if len(opt_e) == 0 else typestr(opt_e[0])\n",
    "\n",
    "        opt.model = getopt('opt.model', str)\n",
    "        opt.task = getopt('opt.task', str)\n",
    "        opt.nch_in = getopt('opt.nch_in', int)\n",
    "        opt.nch_out = getopt('opt.nch_out', int)\n",
    "        opt.n_resgroups = getopt('opt.n_resgroups', int)\n",
    "        opt.n_resblocks = getopt('opt.n_resblocks', int)\n",
    "        opt.n_feats = getopt('opt.n_feats', int)\n",
    "\n",
    "\n",
    "def remove_dataparallel_wrapper(state_dict):\n",
    "    r\"\"\"Converts a DataParallel model to a normal one by removing the \"module.\"\n",
    "    wrapper in the module dictionary\n",
    "\n",
    "    Args:\n",
    "            state_dict: a torch.nn.DataParallel state dictionary\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, vl in state_dict.items():\n",
    "        name = k[7:]  # remove 'module.' of DataParallel\n",
    "        new_state_dict[name] = vl\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def train(dataloader, validloader, net, nepoch=10):\n",
    "    print('Training')\n",
    "    start_epoch = 0\n",
    "    if opt.task == 'segment':\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
    "    loss_function.cuda()\n",
    "    if len(opt.weights) > 0:  # load previous weights?\n",
    "        checkpoint = torch.load(opt.weights)\n",
    "        print('loading checkpoint', opt.weights)\n",
    "        if opt.undomulti:\n",
    "            checkpoint['state_dict'] = remove_dataparallel_wrapper(\n",
    "                checkpoint['state_dict'])\n",
    "        if opt.modifyPretrainedModel:\n",
    "            pretrained_dict = checkpoint['state_dict']\n",
    "            model_dict = net.state_dict()\n",
    "            # 1. filter out unnecessary keys\n",
    "            for k, v in list(pretrained_dict.items()):\n",
    "                print(k)\n",
    "            pretrained_dict = {k: v for k, v in list(\n",
    "                pretrained_dict.items())[:-2]}\n",
    "            # 2. overwrite entries in the existing state dict\n",
    "            model_dict.update(pretrained_dict)\n",
    "            # 3. load the new state dict\n",
    "            net.load_state_dict(model_dict)\n",
    "\n",
    "            # optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "        else:\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            if opt.lr == 1:  # continue as it was\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "\n",
    "\n",
    "        # if opt.modifyPretrainedModel:\n",
    "        #     mod = list(net.children())\n",
    "        #     mod.pop()\n",
    "        #     mod.append(nn.Conv2d(64, 2, 1))\n",
    "        #     net = torch.nn.Sequential(*mod)\n",
    "        #     net.cuda()\n",
    "        #     opt.task = 'segment'\n",
    "\n",
    "    if len(opt.scheduler) > 0:\n",
    "        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=5, min_lr=0, eps=1e-08)\n",
    "        stepsize, gamma = int(opt.scheduler.split(\n",
    "            ',')[0]), float(opt.scheduler.split(',')[1])\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, stepsize, gamma=gamma)\n",
    "        if 'scheduler' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    count = 0\n",
    "    opt.t0 = time.perf_counter()\n",
    "\n",
    "    for epoch in range(start_epoch, nepoch):\n",
    "        mean_loss = 0\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('\\nLearning rate', param_group['lr'])\n",
    "\n",
    "        # if len(opt.lrseq) > 0:\n",
    "        #     t = '[5,1e-4,10,1e-5]'\n",
    "        #     t = np.array(t)\n",
    "        #     epochvec = t[::2].astype('int')\n",
    "        #     lrvec = t[1::2].astype('float')\n",
    "\n",
    "        #     idx = epochvec.indexOf(epoch)\n",
    "        #     opt.lr = lrvec[idx]\n",
    "        #     optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
    "\n",
    "        for i, bat in enumerate(dataloader):\n",
    "            lr, hr = bat[0], bat[1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if opt.model == 'ffdnet':\n",
    "                stdvec = torch.zeros(lr.shape[0])\n",
    "                for j in range(lr.shape[0]):\n",
    "                    noise = lr[j] - hr[j]\n",
    "                    stdvec[j] = torch.std(noise)\n",
    "                noise = net(lr.cuda(), stdvec.cuda())\n",
    "                sr = torch.clamp(lr.cuda() - noise, 0, 1)\n",
    "                gt_noise = lr.cuda() - hr.cuda()\n",
    "                loss = loss_function(noise, gt_noise)\n",
    "            elif opt.task == 'residualdenoising':\n",
    "                noise = net(lr.cuda())\n",
    "                gt_noise = lr.cuda() - hr.cuda()\n",
    "                loss = loss_function(noise, gt_noise)\n",
    "            else:\n",
    "                sr = net(lr.cuda())\n",
    "                if opt.task == 'segment':\n",
    "                    if opt.nch_out > 2:\n",
    "                        hr_classes = torch.round((opt.nch_out+1)*hr).long()\n",
    "                        loss = loss_function(\n",
    "                            sr.squeeze(), hr_classes.squeeze().cuda())\n",
    "                    else:\n",
    "                        loss = loss_function(\n",
    "                            sr.squeeze(), hr.long().squeeze().cuda())\n",
    "                else:\n",
    "                    loss = loss_function(sr, hr.cuda())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ######### Status and display #########\n",
    "            mean_loss += loss.data.item()\n",
    "            print('\\r[%d/%d][%d/%d] Loss: %0.6f' % (epoch+1, nepoch,\n",
    "                                                    i+1, len(dataloader), loss.data.item()), end='')\n",
    "\n",
    "            count += 1\n",
    "            if opt.log and count*opt.batchSize // 1000 > 0:\n",
    "                t1 = time.perf_counter() - opt.t0\n",
    "                mem = torch.cuda.memory_allocated()\n",
    "                print(epoch, count*opt.batchSize, t1, mem,\n",
    "                      mean_loss / count, file=opt.train_stats)\n",
    "                opt.train_stats.flush()\n",
    "                count = 0\n",
    "\n",
    "\n",
    "        # ---------------- Scheduler -----------------\n",
    "        if len(opt.scheduler) > 0:\n",
    "            scheduler.step()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                print('\\nLearning rate', param_group['lr'])\n",
    "                break\n",
    "\n",
    "        # ---------------- Printing -----------------\n",
    "        print('\\nEpoch %d done, %0.6f' %\n",
    "              (epoch, (mean_loss / len(dataloader))))\n",
    "        print('\\nEpoch %d done, %0.6f' %\n",
    "              (epoch, (mean_loss / len(dataloader))), file=opt.fid)\n",
    "        opt.fid.flush()\n",
    "\n",
    "        # ---------------- TEST -----------------\n",
    "        if (epoch + 1) % opt.testinterval == 0:\n",
    "            testAndMakeCombinedPlots(net, validloader, opt, epoch)\n",
    "            # if opt.scheduler:\n",
    "            # scheduler.step(mean_loss / len(dataloader))\n",
    "\n",
    "        if (epoch + 1) % opt.saveinterval == 0:\n",
    "            # torch.save(net.state_dict(), opt.out + '/prelim.pth')\n",
    "            checkpoint = {'epoch': epoch + 1,\n",
    "                          'state_dict': net.state_dict(),\n",
    "                          'optimizer': optimizer.state_dict()}\n",
    "            if len(opt.scheduler) > 0:\n",
    "                checkpoint['scheduler'] = scheduler.state_dict()\n",
    "            torch.save(checkpoint, '%s/prelim%d.pth' % (opt.out, epoch+1))\n",
    "\n",
    "    checkpoint = {'epoch': nepoch,\n",
    "                  'state_dict': net.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict()}\n",
    "    if len(opt.scheduler) > 0:\n",
    "        checkpoint['scheduler'] = scheduler.state_dict()\n",
    "    torch.save(checkpoint, opt.out + '/final.pth')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    try:\n",
    "        os.makedirs(opt.out)\n",
    "    except IOError:\n",
    "        pass\n",
    "\n",
    "    opt.fid = open(opt.out + '/log.txt', 'w')\n",
    "    print(opt)\n",
    "    print(opt, '\\n', file=opt.fid)\n",
    "    print('getting dataloader', opt.root)\n",
    "    dataloader, validloader = GetDataloaders(opt)\n",
    "    net = GetModel(opt)\n",
    "\n",
    "    if opt.log:\n",
    "        opt.train_stats = open(opt.out.replace(\n",
    "            '\\\\', '/') + '/train_stats.csv', 'w')\n",
    "        opt.test_stats = open(opt.out.replace(\n",
    "            '\\\\', '/') + '/test_stats.csv', 'w')\n",
    "        print('iter,nsample,time,memory,meanloss', file=opt.train_stats)\n",
    "        print('iter,time,memory,psnr,ssim', file=opt.test_stats)\n",
    "\n",
    "    import time\n",
    "    t0 = time.perf_counter()\n",
    "    if not opt.test:\n",
    "        print('training')\n",
    "        if opt.model.lower() == 'srgan':\n",
    "            GANtrain(dataloader, validloader, net, nepoch=opt.nepoch)\n",
    "        elif opt.model.lower() == 'esrgan':\n",
    "            ESRGANtrain(dataloader, validloader, net, nepoch=opt.nepoch)\n",
    "        else:\n",
    "            train(dataloader, validloader, net, nepoch=opt.nepoch)\n",
    "        torch.save(net.state_dict(), opt.out + '/final.pth')\n",
    "    else:\n",
    "        if len(opt.weights) > 0:  # load previous weights?\n",
    "            checkpoint = torch.load(opt.weights)\n",
    "            print('loading checkpoint', opt.weights)\n",
    "            if opt.undomulti:\n",
    "                checkpoint['state_dict'] = remove_dataparallel_wrapper(\n",
    "                    checkpoint['state_dict'])\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            print('time: ', time.perf_counter()-t0)\n",
    "        #testAndMakeCombinedPlots(net, validloader, opt)\n",
    "    print('time: ', time.perf_counter()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
