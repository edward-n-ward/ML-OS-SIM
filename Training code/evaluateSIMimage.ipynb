{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"Copy of FixImageSet.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"1zbw9lBJ02Bj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618957605040,"user_tz":-60,"elapsed":437,"user":{"displayName":"edward ward","photoUrl":"","userId":"13037739191521860750"}},"outputId":"1aaeacae-f2f7-45e5-a5b4-1d74f56fafda"},"source":["import numpy as np\n","from tqdm.notebook import tqdm\n","import glob\n","import sys\n","import os\n","from PIL import Image\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"aAtS78kN02Bn"},"source":["import argparse\n","def GetParams():\n","  opt = argparse.Namespace()\n","\n","  opt.model='rcan'#'model to use'\n","  opt.lr = 0.0001 # learning rate\n","  opt.norm = '' # if normalization should not be used\n","  opt.nepoch =100 # number of epochs to train for\n","  opt.saveinterval =1 # number of epochs between saves\n","  opt.modifyPretrainedModel = False\n","  opt.multigpu = False\n","  opt.undomulti = False\n","  opt.ntrain = 5000 # number of samples to train on\n","  opt.scheduler = '' # options for a scheduler, format: stepsize,gamma\n","  opt.log = False\n","  opt.noise ='' # options for noise added, format: poisson,gaussVar\n","\n","  # data\n","  opt.dataset = 'fouriersim' # dataset to train\n","  opt.imageSize = 512 # the low resolution image size\n","  opt.weights = 'D:/User/Edward/Downloads/HPC-download/prelim32.pth' # model to retrain from\n","  opt.basedir = '' # path to prepend to all others paths: root, output, weights\n","  opt.root ='D:/Work/Test datasets/OS-SIM' # dataset to train\n","  opt.server = '' # whether to use server root preset\n","  opt.local = '' # whether to use local root preset: C:/phd-data/datasets/\n","  opt.out = 'D:/Work/Test datasets/OS-SIM/ML-SIM reconstructions/HPC 01-05-2021' # folder to output model training results\n","\n","  # computation \n","  opt.workers  = 1 # number of data loading workers\n","  opt.batchSize = 10 # input batch size\n","\n","  # restoration options\n","  opt.task ='sr' # restoration task \n","  opt.scale = 1 # low to high resolution scaling factor\n","  opt.nch_in = 3 # channels in input \n","  opt.nch_out = 1 # channels in output \n","\n","  # architecture options \n","  opt.narch = 0 # architecture-dependent parameter\n","  opt.n_resblocks  = 2 # number of residual blocks \n","  opt.n_resgroups  = 3 # number of residual groups \n","  opt.reduction  = 2 # number of 36eature maps\n","  opt.n_feats = 54 \n","\n","  # test options\n","  opt.ntest  = 10 # number of images to test per epoch or test run \n","  opt.testinterval  = 1 # number of epochs between tests during training \n","  opt.test = False\n","  opt.cpu = False # not supported for training\n","  opt.batchSize_test  = 1 # input batch size for test loader \n","  opt.plotinterval  = 1 # number of test samples between plotting \n","    \n","  return opt"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b08oUqD0qYPP","executionInfo":{"status":"ok","timestamp":1618957608191,"user_tz":-60,"elapsed":3573,"user":{"displayName":"edward ward","photoUrl":"","userId":"13037739191521860750"}},"outputId":"549bc593-c0df-446b-8650-ad0ac29ca1fe"},"source":["\n","import math\n","import os\n","\n","import torch\n","import time \n","\n","import torch.optim as optim\n","import torchvision\n","from torch.autograd import Variable\n","\n","from skimage import io\n","from models import *\n","from datahandler import *\n","\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import glob\n","\n","def remove_dataparallel_wrapper(state_dict):\n","\tr\"\"\"Converts a DataParallel model to a normal one by removing the \"module.\"\n","\twrapper in the module dictionary\n","\n","\tArgs:\n","\t\tstate_dict: a torch.nn.DataParallel state dictionary\n","\t\"\"\"\n","\tfrom collections import OrderedDict\n","\n","\tnew_state_dict = OrderedDict()\n","\tfor k, vl in state_dict.items():\n","\t\tname = k[7:] # remove 'module.' of DataParallel\n","\t\tnew_state_dict[name] = vl\n","\n","\treturn new_state_dict\n","\n","\n","def changeColour(I): # change colours (used to match WEKA output)\n","    Inew = np.zeros(I.shape + (3,)).astype('uint8')\n","    for rowidx in range(I.shape[0]):\n","        for colidx in range(I.shape[1]):\n","            if I[rowidx][colidx] == 0:\n","                Inew[rowidx][colidx] = [198,118,255]\n","            elif I[rowidx][colidx] == 127:\n","                Inew[rowidx][colidx] = [79,255,130]\n","            elif I[rowidx][colidx] == 255:\n","                Inew[rowidx][colidx] = [255,0,0]\n","    return Inew\n","\n","\n","def loadimg(imgfile):\n","    stack = io.imread(imgfile)\n","    inputimgs,wfimgs = [],[]\n","\n","    for i in range(int(len(stack)/9)):\n","        inputimg = stack[i*9:(i+1)*9]\n","\n","        if inputimg.shape[1] != 512 or inputimg.shape[2] != 512:\n","            print(imgfile,'not 512x512! Cropping')\n","            inputimg = inputimg[:,:512,:512]\n","                 \n","        widefield = np.mean(inputimg,0)\n","        widefield = (widefield - np.min(widefield)) / (np.max(widefield) - np.min(widefield))    \n","\n","\n","        if opt.norm == 'convert': # raw img from microscope, needs normalisation and correct frame ordering\n","            print('Raw input assumed - converting')\n","\n","\n","            inputimg = np.rot90(inputimg,axes=(1,2))\n","            inputimg = inputimg[[6,7,8,3,4,5,0,1,2]] # could also do [8,7,6,5,4,3,2,1,0]\n","            for i in range(len(inputimg)):\n","                inputimg[i] = 100 / np.max(inputimg[i]) * inputimg[i]\n","            inputimg = torch.tensor(inputimg.astype('float') / 255).float()\n","        elif 'convert' in opt.norm:\n","            fac = float(opt.norm[7:])\n","            inputimg = np.rot90(inputimg,axes=(1,2))\n","            inputimg = inputimg[[6,7,8,3,4,5,0,1,2]] # could also do [8,7,6,5,4,3,2,1,0]\n","            for i in range(len(inputimg)):\n","                inputimg[i] = fac * 255 / np.max(inputimg[i]) * inputimg[i]\n","            inputimg = torch.tensor(inputimg.astype('float') / 255).float()\n","        elif opt.norm == 'minmax':\n","            inputimg = torch.tensor(inputimg.astype('float') / 255).float()\n","            for i in range(len(inputimg)):\n","                inputimg[i] = (inputimg[i] - torch.min(inputimg[i])) / (torch.max(inputimg[i]) - torch.min(inputimg[i]))\n","\n","        else:\n","            inputimg = torch.tensor(inputimg.astype('float') / 255).float()\n","\n","\n","        widefield = torch.tensor(widefield).float()\n","        \n","        inputimgs.append(inputimg)\n","        wfimgs.append(widefield)\n","\n","    return inputimgs,wfimgs\n","\n","\n","def EvaluateModel(opt):\n","\n","    try:\n","        os.makedirs(opt.out)\n","    except IOError:\n","        pass\n","\n","    opt.fid = open(opt.out + '/log.txt','w')\n","    print(opt)\n","    print(opt,'\\n',file=opt.fid)\n","    \n","    net = GetModel(opt)\n","\n","    checkpoint = torch.load(opt.weights)\n","    if opt.cpu:\n","        net.cpu()\n","    \n","    print('loading checkpoint',opt.weights)\n","    if opt.undomulti:\n","        checkpoint['state_dict'] = remove_dataparallel_wrapper(checkpoint['state_dict'])\n","    net.load_state_dict(checkpoint['state_dict'])\n","\n","    if opt.root.split('.')[-1] == 'png' or opt.root.split('.')[-1] == 'jpg':\n","        imgs = [opt.root]\n","    else:\n","        imgs = []\n","        imgs.extend(glob.glob(opt.root + '/*.jpg'))\n","        imgs.extend(glob.glob(opt.root + '/*.png'))\n","        imgs.extend(glob.glob(opt.root + '/*.tif'))\n","        if len(imgs) == 0: # scan everything\n","            imgs.extend(glob.glob(opt.root + '/**/*.jpg',recursive=True))\n","            imgs.extend(glob.glob(opt.root + '/**/*.png',recursive=True))\n","            imgs.extend(glob.glob(opt.root + '/**/*.tif',recursive=True))\n","\n","    imageSize = opt.imageSize\n","\n","    for i, imgfile in enumerate(imgs):\n","        print('\\rProcessing image [%d/%d]' % (i+1,len(imgs)),end='')\n","        img = io.imread(imgfile)\n","        img = np.array(img)\n","        img = img/np.amax(img)\n","\n","        nImgs = stack.shape[0] // opt.nch_in\n","        sr = np.zeros([512,512,nImgs]) \n","        wf = np.zeros([512,512,nImgs])\n","\n","        if img.shape[1] != 512:\n","            print('\\rimage', imgfile,' is not 512x512! Cropping')\n","            img = img[:,:512,:512]\n","        for stack_idx in tqdm(range(stack.shape[0] // opt.nch_in)):\n","                stackSubset = stack[stack_idx*opt.nch_in:(stack_idx+1)*opt.nch_in]\n","\n","        frames = np.zeros([512,512,nImgs])\n","        for stack_idx in tqdm(range(nImgs // opt.nch_in)):\n","            stackSubset = stack[stack_idx*opt.nch_in:(stack_idx+1)*opt.nch_in] \n","            \n","            sub_tensor = toTensor(stackSubset)\n","            sub_tensor = sub_tensor.unsqueeze(0)\n","            sub_tensor = sub_tensor.type(torch.FloatTensor)\n","            wf[:,:,stack_idx] = np.mean(stackSubset,0)\n","            \n","            with torch.no_grad():\n","                if opt.cpu:\n","                    sr = net(sub_tensor)\n","                else:\n","                    sr = net(sub_tensor.cuda())\n","                sr = sr.cpu()\n","\n","                sr = torch.clamp(sr[0],0,1)\n","                sr_frame = sr.numpy()\n","                sr_frame = np.squeeze(sr_frame)\n","                sr[:,:,idx] = sr_frame\n","                               \n","            \n","        frames = (frames * 32000).astype('uint16')\n","        if nImgs > 1:\n","            frames = np.moveaxis(frames,2,0)\n","        \n","        svPath = imgfile[:-4] + '_sr.tif'\n","        io.imsave(svPath,frames)\n","\n","\n","\n","if __name__ == '__main__':\n","    opt = GetParams()\n","\n","    EvaluateModel(opt)\n"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(basedir='', batchSize=10, batchSize_test=1, cpu=False, dataset='fouriersim', fid=<_io.TextIOWrapper name='D:/Work/Test datasets/OS-SIM/ML-SIM reconstructions/HPC 01-05-2021/log.txt' mode='w' encoding='cp1252'>, imageSize=512, local='', log=False, lr=0.0001, model='rcan', modifyPretrainedModel=False, multigpu=False, n_feats=54, n_resblocks=2, n_resgroups=3, narch=0, nch_in=3, nch_out=1, nepoch=100, noise='', norm='', ntest=10, ntrain=5000, out='D:/Work/Test datasets/OS-SIM/ML-SIM reconstructions/HPC 01-05-2021', plotinterval=1, reduction=2, root='D:/Work/Test datasets/OS-SIM', saveinterval=1, scale=1, scheduler='', server='', task='sr', test=False, testinterval=1, undomulti=False, weights='D:/User/Edward/Downloads/HPC-download/prelim32.pth', workers=1)\n","not using normalization\n","loading checkpoint D:/User/Edward/Downloads/HPC-download/prelim32.pth\n","  0%|          | 0/30 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"axes don't match array","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-9-3cdc3e04f4c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mEvaluateModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-9-3cdc3e04f4c9>\u001b[0m in \u001b[0;36mEvaluateModel\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[0mpil_sub_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0msub_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpil_sub_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m             \u001b[0msub_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[0msub_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# handle numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m# backward compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: axes don't match array"]}]},{"cell_type":"markdown","metadata":{"id":"AJV3IDmZ1DE-"},"source":["# New section"]}]}